

oth:
  main_yaml:
    # basic_with_zero_out

    # Training parameters

    # More situationally changed parameters
    is_resource_calc_ready: True
    is_pruning_ready: False   # set to false to skip all prun
    path_to_data: "./Data/sclera_data"
    target: "scleras"
    train_epoch_size: 200    # we use random resampling, so this is unconnected with train_dataset size
    # In patchification the actual size in train is num_of_patches_from_img * train_epoch_size
    val_epoch_size: 30
    test_epoch_size: 20
    train_batch_size: 5    # limits 6, 17
    eval_batch_size: 10   # In patchification, this doesn't matter, because VRAM is limited by input_size_limit
    learning_rate: 0.0001
    num_of_dataloader_workers: 10
    # train_epoch_size_limit: 400    # remnant of old time. When no random sampling, with a dataset size of 1000 that was a bit much for every epoch. So we just limited it.

    # More trainig-type-defining parameters
    cleanup_k: 1      # Options: 0, 1, 2, 3
    optimizer_used: "Adam"    # Options: Adam, SGD, LBFGS
    zero_out_non_sclera_on_predictions: False
    loss_fn_name: "WFTL"
    loss_params: 
      fp_imp: 0.5
      fn_imp: 0.5
      gamma: 2
      weights_list: [1, 1, 1.8]
      ignore_classes: [0] # ignore the background in loss

    # Model parameters that need to be set in stone for pruning
    model: "64_2_4"



    # --------- Overrides ---------

    # Dataset parameters
    dataset_type: simple    # Options: simple, vasd, multi
    aug_type: pass    # Options: tf, np, pass
    metrics_aggregation_fn: mean_no_background
    zero_out_non_sclera: False
    add_sclera_to_img: False
    add_bcosfire_to_img: True
    add_coye_to_img: True
