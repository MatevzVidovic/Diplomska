Validation phase: False
Namespace(iter_possible_stop=999999, BATCH_SIZE=4, NUM_OF_DATALOADER_WORKERS=10, NUM_TRAIN_ITERS_BETWEEN_PRUNINGS=1, SAVE_DIR='UNet', PATH_TO_DATA='./sclera_data', is_test_run=False, pruning_phase=False, pbop=False, e_ix=3, mti=2, map=1000000000.0, nept=1, pnkao=20, nftp=1, rn='flops_num', ptp=0.01)
path to file: ./sclera_data
summary for train
valid images: 1288
summary for val
valid images: 344
summary for test
valid images: 208
train dataset len: 1288
val dataset len: 344
test dataset len: 208
train dataloader num of batches: 322
val dataloader num of batches: 86
test dataloader num of batches: 52
Device: cuda
loss: 0.145015  [    4/ 1288]
loss: 0.160650  [    8/ 1288]
loss: 0.141204  [   12/ 1288]
loss: 0.127558  [   16/ 1288]
loss: 0.126745  [   20/ 1288]
loss: 0.128997  [   24/ 1288]
loss: 0.132281  [   28/ 1288]
loss: 0.129392  [   32/ 1288]
loss: 0.136351  [   36/ 1288]
loss: 0.140374  [   40/ 1288]
loss: 0.162235  [   44/ 1288]
loss: 0.136911  [   48/ 1288]
loss: 0.133765  [   52/ 1288]
loss: 0.146704  [   56/ 1288]
loss: 0.133288  [   60/ 1288]
loss: 0.112703  [   64/ 1288]
loss: 0.123960  [   68/ 1288]
loss: 0.131090  [   72/ 1288]
loss: 0.132221  [   76/ 1288]
loss: 0.132969  [   80/ 1288]
loss: 0.135999  [   84/ 1288]
loss: 0.120082  [   88/ 1288]
loss: 0.138624  [   92/ 1288]
loss: 0.174317  [   96/ 1288]
loss: 0.147478  [  100/ 1288]
loss: 0.130671  [  104/ 1288]
loss: 0.121195  [  108/ 1288]
loss: 0.119094  [  112/ 1288]
loss: 0.156638  [  116/ 1288]
loss: 0.110244  [  120/ 1288]
loss: 0.132963  [  124/ 1288]
loss: 0.107319  [  128/ 1288]
loss: 0.119139  [  132/ 1288]
loss: 0.137637  [  136/ 1288]
loss: 0.119614  [  140/ 1288]
loss: 0.110528  [  144/ 1288]
loss: 0.146012  [  148/ 1288]
loss: 0.141545  [  152/ 1288]
loss: 0.164903  [  156/ 1288]
loss: 0.121106  [  160/ 1288]
loss: 0.156229  [  164/ 1288]
loss: 0.116412  [  168/ 1288]
loss: 0.121810  [  172/ 1288]
loss: 0.143497  [  176/ 1288]
loss: 0.142173  [  180/ 1288]
loss: 0.140253  [  184/ 1288]
loss: 0.135240  [  188/ 1288]
loss: 0.129248  [  192/ 1288]
loss: 0.116825  [  196/ 1288]
loss: 0.125265  [  200/ 1288]
loss: 0.136758  [  204/ 1288]
loss: 0.114566  [  208/ 1288]
loss: 0.114074  [  212/ 1288]
loss: 0.119510  [  216/ 1288]
loss: 0.227361  [  220/ 1288]
loss: 0.158076  [  224/ 1288]
loss: 0.148799  [  228/ 1288]
loss: 0.110594  [  232/ 1288]
loss: 0.117821  [  236/ 1288]
loss: 0.110540  [  240/ 1288]
loss: 0.122888  [  244/ 1288]
loss: 0.143126  [  248/ 1288]
loss: 0.132896  [  252/ 1288]
loss: 0.116796  [  256/ 1288]
loss: 0.114370  [  260/ 1288]
loss: 0.107146  [  264/ 1288]
loss: 0.130772  [  268/ 1288]
loss: 0.141844  [  272/ 1288]
loss: 0.114545  [  276/ 1288]
loss: 0.126155  [  280/ 1288]
loss: 0.128422  [  284/ 1288]
loss: 0.121157  [  288/ 1288]
loss: 0.128533  [  292/ 1288]
loss: 0.124973  [  296/ 1288]
loss: 0.119532  [  300/ 1288]
loss: 0.118269  [  304/ 1288]
loss: 0.114955  [  308/ 1288]
loss: 0.130923  [  312/ 1288]
loss: 0.115872  [  316/ 1288]
loss: 0.121599  [  320/ 1288]
loss: 0.133084  [  324/ 1288]
loss: 0.146089  [  328/ 1288]
loss: 0.121093  [  332/ 1288]
loss: 0.113787  [  336/ 1288]
loss: 0.130950  [  340/ 1288]
loss: 0.123954  [  344/ 1288]
loss: 0.132937  [  348/ 1288]
loss: 0.140119  [  352/ 1288]
loss: 0.150260  [  356/ 1288]
loss: 0.116801  [  360/ 1288]
loss: 0.110278  [  364/ 1288]
loss: 0.113277  [  368/ 1288]
loss: 0.123728  [  372/ 1288]
loss: 0.108982  [  376/ 1288]
loss: 0.109862  [  380/ 1288]
loss: 0.111909  [  384/ 1288]
loss: 0.157007  [  388/ 1288]
loss: 0.115853  [  392/ 1288]
loss: 0.117926  [  396/ 1288]
loss: 0.116515  [  400/ 1288]
loss: 0.111970  [  404/ 1288]
loss: 0.119464  [  408/ 1288]
loss: 0.158902  [  412/ 1288]
loss: 0.100453  [  416/ 1288]
loss: 0.120783  [  420/ 1288]
loss: 0.128160  [  424/ 1288]
loss: 0.116249  [  428/ 1288]
loss: 0.110149  [  432/ 1288]
loss: 0.105402  [  436/ 1288]
loss: 0.129692  [  440/ 1288]
loss: 0.131326  [  444/ 1288]
loss: 0.174517  [  448/ 1288]
loss: 0.107788  [  452/ 1288]
loss: 0.112254  [  456/ 1288]
loss: 0.147949  [  460/ 1288]
loss: 0.101064  [  464/ 1288]
loss: 0.184982  [  468/ 1288]
loss: 0.125740  [  472/ 1288]
loss: 0.102407  [  476/ 1288]
loss: 0.102559  [  480/ 1288]
loss: 0.111654  [  484/ 1288]
loss: 0.129719  [  488/ 1288]
loss: 0.125739  [  492/ 1288]
loss: 0.128116  [  496/ 1288]
loss: 0.099652  [  500/ 1288]
loss: 0.109477  [  504/ 1288]
loss: 0.122895  [  508/ 1288]
loss: 0.112118  [  512/ 1288]
loss: 0.123196  [  516/ 1288]
loss: 0.134639  [  520/ 1288]
loss: 0.121588  [  524/ 1288]
loss: 0.113622  [  528/ 1288]
loss: 0.122226  [  532/ 1288]
loss: 0.111948  [  536/ 1288]
loss: 0.129659  [  540/ 1288]
loss: 0.116075  [  544/ 1288]
loss: 0.149151  [  548/ 1288]
loss: 0.104363  [  552/ 1288]
loss: 0.105359  [  556/ 1288]
loss: 0.105201  [  560/ 1288]
loss: 0.116190  [  564/ 1288]
loss: 0.109854  [  568/ 1288]
loss: 0.108820  [  572/ 1288]
loss: 0.111233  [  576/ 1288]
loss: 0.126566  [  580/ 1288]
loss: 0.101766  [  584/ 1288]
loss: 0.097613  [  588/ 1288]
loss: 0.102007  [  592/ 1288]
loss: 0.142839  [  596/ 1288]
loss: 0.131464  [  600/ 1288]
loss: 0.104137  [  604/ 1288]
loss: 0.137234  [  608/ 1288]
loss: 0.124538  [  612/ 1288]
loss: 0.107995  [  616/ 1288]
loss: 0.132248  [  620/ 1288]
loss: 0.111395  [  624/ 1288]
loss: 0.103220  [  628/ 1288]
loss: 0.129648  [  632/ 1288]
loss: 0.101663  [  636/ 1288]
loss: 0.124927  [  640/ 1288]
loss: 0.107803  [  644/ 1288]
loss: 0.115327  [  648/ 1288]
loss: 0.108415  [  652/ 1288]
loss: 0.150354  [  656/ 1288]
loss: 0.101730  [  660/ 1288]
loss: 0.100265  [  664/ 1288]
loss: 0.102227  [  668/ 1288]
loss: 0.109115  [  672/ 1288]
loss: 0.123301  [  676/ 1288]
loss: 0.142173  [  680/ 1288]
loss: 0.118684  [  684/ 1288]
loss: 0.109441  [  688/ 1288]
loss: 0.111653  [  692/ 1288]
loss: 0.119001  [  696/ 1288]
loss: 0.115571  [  700/ 1288]
loss: 0.104015  [  704/ 1288]
loss: 0.101947  [  708/ 1288]
loss: 0.137964  [  712/ 1288]
loss: 0.111428  [  716/ 1288]
loss: 0.140732  [  720/ 1288]
loss: 0.114822  [  724/ 1288]
loss: 0.120142  [  728/ 1288]
loss: 0.100490  [  732/ 1288]
loss: 0.100777  [  736/ 1288]
loss: 0.109540  [  740/ 1288]
loss: 0.146099  [  744/ 1288]
loss: 0.105202  [  748/ 1288]
loss: 0.120121  [  752/ 1288]
loss: 0.192211  [  756/ 1288]
loss: 0.109436  [  760/ 1288]
loss: 0.106964  [  764/ 1288]
loss: 0.102337  [  768/ 1288]
loss: 0.108189  [  772/ 1288]
loss: 0.103252  [  776/ 1288]
loss: 0.117386  [  780/ 1288]
loss: 0.106481  [  784/ 1288]
loss: 0.116643  [  788/ 1288]
loss: 0.106105  [  792/ 1288]
loss: 0.109567  [  796/ 1288]
loss: 0.102205  [  800/ 1288]
loss: 0.109628  [  804/ 1288]
loss: 0.111460  [  808/ 1288]
loss: 0.114438  [  812/ 1288]
loss: 0.115484  [  816/ 1288]
loss: 0.097426  [  820/ 1288]
loss: 0.100382  [  824/ 1288]
loss: 0.116098  [  828/ 1288]
loss: 0.105042  [  832/ 1288]
loss: 0.139012  [  836/ 1288]
loss: 0.113604  [  840/ 1288]
loss: 0.107262  [  844/ 1288]
loss: 0.102043  [  848/ 1288]
loss: 0.094319  [  852/ 1288]
loss: 0.105773  [  856/ 1288]
loss: 0.098261  [  860/ 1288]
loss: 0.096404  [  864/ 1288]
loss: 0.110692  [  868/ 1288]
loss: 0.106818  [  872/ 1288]
loss: 0.102591  [  876/ 1288]
loss: 0.089492  [  880/ 1288]
loss: 0.100273  [  884/ 1288]
loss: 0.115652  [  888/ 1288]
loss: 0.127259  [  892/ 1288]
loss: 0.098958  [  896/ 1288]
loss: 0.103182  [  900/ 1288]
loss: 0.120288  [  904/ 1288]
loss: 0.097151  [  908/ 1288]
loss: 0.101535  [  912/ 1288]
loss: 0.112799  [  916/ 1288]
loss: 0.116179  [  920/ 1288]
loss: 0.096238  [  924/ 1288]
loss: 0.103295  [  928/ 1288]
loss: 0.096799  [  932/ 1288]
loss: 0.104983  [  936/ 1288]
loss: 0.109068  [  940/ 1288]
loss: 0.114186  [  944/ 1288]
loss: 0.105345  [  948/ 1288]
loss: 0.108377  [  952/ 1288]
loss: 0.108571  [  956/ 1288]
loss: 0.106550  [  960/ 1288]
loss: 0.125648  [  964/ 1288]
loss: 0.097861  [  968/ 1288]
loss: 0.107027  [  972/ 1288]
loss: 0.133362  [  976/ 1288]
loss: 0.120109  [  980/ 1288]
loss: 0.102602  [  984/ 1288]
loss: 0.102917  [  988/ 1288]
loss: 0.107382  [  992/ 1288]
loss: 0.110635  [  996/ 1288]
loss: 0.096257  [ 1000/ 1288]
loss: 0.102906  [ 1004/ 1288]
loss: 0.099171  [ 1008/ 1288]
loss: 0.113455  [ 1012/ 1288]
loss: 0.096586  [ 1016/ 1288]
loss: 0.105301  [ 1020/ 1288]
loss: 0.098489  [ 1024/ 1288]
loss: 0.099698  [ 1028/ 1288]
loss: 0.114057  [ 1032/ 1288]
loss: 0.092151  [ 1036/ 1288]
loss: 0.099920  [ 1040/ 1288]
loss: 0.093023  [ 1044/ 1288]
loss: 0.101052  [ 1048/ 1288]
loss: 0.123684  [ 1052/ 1288]
loss: 0.103364  [ 1056/ 1288]
loss: 0.103006  [ 1060/ 1288]
loss: 0.093472  [ 1064/ 1288]
loss: 0.139275  [ 1068/ 1288]
loss: 0.093085  [ 1072/ 1288]
loss: 0.118104  [ 1076/ 1288]
loss: 0.126438  [ 1080/ 1288]
loss: 0.098900  [ 1084/ 1288]
loss: 0.102778  [ 1088/ 1288]
loss: 0.100300  [ 1092/ 1288]
loss: 0.114247  [ 1096/ 1288]
loss: 0.097913  [ 1100/ 1288]
loss: 0.106037  [ 1104/ 1288]
loss: 0.121608  [ 1108/ 1288]
loss: 0.124365  [ 1112/ 1288]
loss: 0.105961  [ 1116/ 1288]
loss: 0.094832  [ 1120/ 1288]
loss: 0.103493  [ 1124/ 1288]
loss: 0.115681  [ 1128/ 1288]
loss: 0.116062  [ 1132/ 1288]
loss: 0.116426  [ 1136/ 1288]
loss: 0.113886  [ 1140/ 1288]
loss: 0.110036  [ 1144/ 1288]
loss: 0.101300  [ 1148/ 1288]
loss: 0.092588  [ 1152/ 1288]
loss: 0.103377  [ 1156/ 1288]
loss: 0.140463  [ 1160/ 1288]
loss: 0.179049  [ 1164/ 1288]
loss: 0.122069  [ 1168/ 1288]
loss: 0.104526  [ 1172/ 1288]
loss: 0.097094  [ 1176/ 1288]
loss: 0.099452  [ 1180/ 1288]
loss: 0.159020  [ 1184/ 1288]
loss: 0.100683  [ 1188/ 1288]
loss: 0.101176  [ 1192/ 1288]
loss: 0.109752  [ 1196/ 1288]
loss: 0.137479  [ 1200/ 1288]
loss: 0.114079  [ 1204/ 1288]
loss: 0.099313  [ 1208/ 1288]
loss: 0.101799  [ 1212/ 1288]
loss: 0.096775  [ 1216/ 1288]
loss: 0.092859  [ 1220/ 1288]
loss: 0.114621  [ 1224/ 1288]
loss: 0.114883  [ 1228/ 1288]
loss: 0.107566  [ 1232/ 1288]
loss: 0.103453  [ 1236/ 1288]
loss: 0.109737  [ 1240/ 1288]
loss: 0.094697  [ 1244/ 1288]
loss: 0.101453  [ 1248/ 1288]
loss: 0.110939  [ 1252/ 1288]
loss: 0.098411  [ 1256/ 1288]
loss: 0.105425  [ 1260/ 1288]
loss: 0.095940  [ 1264/ 1288]
loss: 0.098431  [ 1268/ 1288]
loss: 0.096967  [ 1272/ 1288]
loss: 0.094520  [ 1276/ 1288]
loss: 0.093791  [ 1280/ 1288]
loss: 0.101240  [ 1284/ 1288]
loss: 0.114434  [ 1288/ 1288]
validation Error: 
 Avg loss: 0.09810604 
 IoU: 0.974642 
 F1: 0.917748 

test Error: 
 Avg loss: 0.09771690 
 IoU: 0.977174 
 F1: 0.917439 

Traceback (most recent call last):
  File "/home/matevzvidovic/Desktop/Diplomska/Prototip/Delo/main.py", line 933, in <module>
    train_automatically(model_wrapper, main_save_path, validation_stop, max_training_iters=max_train_iters, max_auto_prunings=max_auto_prunings, train_iter_possible_stop=iter_possible_stop, pruning_phase=is_pruning_ph, error_ix=err_ix,
  File "/home/matevzvidovic/Desktop/Diplomska/Prototip/Delo/training_support.py", line 515, in train_automatically
    training_logs, pruning_logs = perform_auto_save(model_wrapper, training_logs, pruning_logs, main_save_path, val_error, test_error, train_iter, curr_training_phase_serial_num)
  File "/home/matevzvidovic/Desktop/Diplomska/Prototip/Delo/training_support.py", line 303, in perform_auto_save
    training_logs.delete_all_but_best_k_models(3, model_wrapper)
  File "/home/matevzvidovic/Desktop/Diplomska/Prototip/Delo/training_support.py", line 169, in delete_all_but_best_k_models
    model_path = error[3]
TypeError: 'NoneType' object is not subscriptable
