


# Docs:
# ctrl F for them. I will not keep them in the right order.


# ----------------------------------------------------------------------------------------------------
# batch_size: 4
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# num_of_dataloader_workers: 7
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# train_epoch_size_limit: 400
If we have 1500 images in the training set, and we set this to 1000, 
we will stop the epoch as we have trained on >= 1000 images.
We should watch out to use shuffle=True in the DataLoader, because otherwise we will always only train on the first 1000 images in the Dataset's ordering.

This is useful if we want to do rapid prototyping, and we don't want to wait for the whole epoch to finish.
Or if one epoch is too huge so it just makes more sense to train on a few smaller ones.
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# num_epochs_per_training_iteration: 1
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# cleanup_k: 3
When saving, how many models to keep. (k+1 models are kept if the current model is not in the best k - because we have to keep it to train the next one.)")
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# dataset_option: "augment"
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# optimizer_used: "Adam"
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# loss_fn_name: "MCDL"
MCDL for MultiClassDiceLoss, CE for CrossEntropyLoss,
CEHW for CrossEntropyLoss_hardcode_weighted,
MCDL_CEHW_W for MultiClassDiceLoss and CrossEntropyLoss_hardcode_weighted in a weighted pairing of both losses,
MCDLW for MultiClassDiceLoss with background adjustment
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# alphas: []
Alphas used in loss_fn. Currently at most one is used, so just pass list of size 1, like:
alphas: [0.8]
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# model: "64_2_6"
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# input_width: 2048
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# input_height: 1024
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# input_channels: 3
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# output_channels: 2
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# num_train_iters_between_prunings: 10
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# max_auto_prunings: 1000000000
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# proportion_to_prune: 0.01
Proportion of original {resource_name} to prune - actually, we don't just prune by this percent, because that get's us bad results.
Every time we prune, we prune e.g. 1 percent. Because of pnkao we overshoot by a little. 
So next time, if we prune by 1 percent again, we will overshoot by a little again, and the overshoots compound.
So we will instead prune in this way: get in which bracket of this percent we are so far (eg, we have 79.9 percent of original weights), 
then we will prune to 79 percent and pnkao will overshoot a little.
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# cleaning_error_ix: 0        # Options: 0, 1, 2, 3
We take ix of the loss you want in the tuple: (avg_loss, approx_IoU, F1, IoU)
avg_loss only really makes sense - because if the model is a bit better, we are sure it goes down
(IoU and F1 are based on binary classification, so a slightly better model might still do the same predictions, 
so the loss would be the same - and so you can't choose which to clean away)
# ....................................................................................................




# ----------------------------------------------------------------------------------------------------  
# prune_by_original_percent: false
Prune by original percent, otherwise by number of filters
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# num_filters_to_prune: 1
!!! ONLY APPLIES IF --pbop IS FALSE !!!
Number of filters to prune in one pruning""")
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# prune_n_kernels_at_once: 20
!!! THIS IS OVERRIDDEN IF --ifn IS 0 OR 1 !!!
It becomes 1. Because we need the new importances to be calculated after every pruning.

Prune n kernels at once - in one pruning iteration, we:
1. calculate the importance of all kernels
2. prune n kernels based on these importances
3. calculate the importances based on the new pruned model
4. prune n kernels based on these new importances
5. ...
Repeat until we have pruned the desired amount of kernels.
Then we go back to training the model until it is time for another pruning iteration.

for --pbop:
As we are pruning n kernels, we actually stop doing so if we have met the goal resource requirement.

So the only function of pnkao is how often we calculate the importances. There is no overshooting in terms of resources.
So we don't actually prune n kernels at once any more.
This is outdated nomenclature, but I will keep it for now, because changing it would be hell, and it is roughly correct anyway.

In theory, it would be best to have --pnkao at 1, because we get the most accurate importance values.
However, this is very slow. And also it doesn't make that much of a difference in quality.
(would be better to do an actual retraining between the prunings then, 
since we are doing so many epoch passes it is basically computationally worse than retraining).


If you are not using --pbop, then you can set --pnkao to 1e9.
Because the number of kernels we actually prune is capped by --nftp.
It will look like this:
1. calculate the importance of all kernels
2. prune --nftp kernels based on these importances
Done.
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# resource_name_to_prune_by: "flops_num"        # Options: flops_num, weights_num, kernels_num
Resource name to prune by.
# ....................................................................................................



# ----------------------------------------------------------------------------------------------------
# importance_func: [0.5, 0.5, 0.5]        # [0.5, 0.5, 0.5], 0, 1
If 0, random pruning. If 1, uniform pruning. If tuple, we get IPAD with those 3 alphas.")
# ....................................................................................................




